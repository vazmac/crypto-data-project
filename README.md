# ğŸš€ Crypto DataOps Pipeline: End-to-End Analytics Engineering

![DataOps](https://img.shields.io/badge/Methodology-DataOps-blue)
![Airflow](https://img.shields.io/badge/Orchestrator-Airflow_2.8.1-017CEE?logo=apacheairflow)
![dbt](https://img.shields.io/badge/Transformation-dbt_core-FF694B?logo=dbt)
![PostgreSQL](https://img.shields.io/badge/Database-PostgreSQL-4169E1?logo=postgresql)
![Docker](https://img.shields.io/badge/Infrastructure-Docker-2496ED?logo=docker)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?logo=githubactions)

## ğŸ“Œ VisÃ£o Geral do Projeto
Este projeto implementa uma arquitetura moderna de dados (Modern Data Stack) para extraÃ§Ã£o, transformaÃ§Ã£o e orquestraÃ§Ã£o de mÃ©tricas do mercado de criptomoedas (CoinGecko). ConstruÃ­do com uma mentalidade rigorosa de **DataOps**, o pipeline foca-se na escalabilidade, reprodutibilidade e isolamento de ambientes.

## ğŸ—ï¸ Arquitetura e Fluxo de Dados (ETL/ELT)

1. **ExtraÃ§Ã£o (Python + Airflow):** Scripts em Python extraem dados da API e carregam-nos para o Data Lake (MinIO) e para a camada `raw` no PostgreSQL.
2. **TransformaÃ§Ã£o (dbt):** - **Silver Layer (`staging`):** Limpeza e normalizaÃ§Ã£o dos dados (Materializados como `views`).
   - **Gold Layer (`marts`):** CriaÃ§Ã£o de tabelas de factos e dimensÃµes para reporting, como `fct_crypto_daily_metrics` (Materializados como `tables`).
   - *Nota:* UtilizaÃ§Ã£o de uma macro customizada (`generate_schema_name`) para garantir a escrita limpa nos schemas de destino, sem prefixos.
3. **ExportaÃ§Ã£o:** GeraÃ§Ã£o de um ficheiro `.csv` dinÃ¢mico isolado num volume local (`/exports`) para consumo seguro em ferramentas de BI (PowerBI).

## ğŸ§  Boas PrÃ¡ticas de DataOps Implementadas

- **Infraestrutura como CÃ³digo (IaC):** Todo o ambiente Ã© levantado via `docker-compose.yml` com mapeamento rigoroso de volumes.
- **MicroserviÃ§os:** Airflow dividido em processos independentes (`Webserver` e `Scheduler`) para garantir resiliÃªncia e evitar falhas em cascata.
- **Dependency Pinning:** Ficheiro `requirements.txt` blindado (ex: `apache-airflow==2.8.1`) para evitar quebras por atualizaÃ§Ãµes silenciosas (Dependency Hell).
- **SeparaÃ§Ã£o de PreocupaÃ§Ãµes (SoC):** CÃ³digo de orquestraÃ§Ã£o (`dags/`) e cÃ³digo de transformaÃ§Ã£o (`coingecko_dw/`) vivem em diretÃ³rios paralelos para otimizar o *parsing* do Airflow.
- **IntegraÃ§Ã£o ContÃ­nua (CI):** Pipeline configurado no GitHub Actions para levantar um Postgres efÃ©mero e validar a compilaÃ§Ã£o do dbt (`dbt compile`) em cada *Push*/*Pull Request*.

## ğŸ“‚ Estrutura do RepositÃ³rio

```text
â”œâ”€â”€ .github/workflows/      # Pipelines de CI/CD (GitHub Actions)
â”œâ”€â”€ coingecko_dw/           # Projeto dbt (TransformaÃ§Ã£o de Dados)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/        # Modelos da camada Silver
â”‚   â”‚   â””â”€â”€ marts/          # Modelos da camada Gold
â”‚   â””â”€â”€ macros/             # Macros Jinja (ex: custom schema names)
â”œâ”€â”€ dags/                   # DAGs do Airflow (OrquestraÃ§Ã£o e Python Scripts)
â”œâ”€â”€ exports/                # Volume isolado com os outputs (CSVs) para o PowerBI
â”œâ”€â”€ docker-compose.yml      # Infraestrutura (Postgres, MinIO, Airflow)
â”œâ”€â”€ Dockerfile              # Imagem customizada do Airflow com dependÃªncias
â”œâ”€â”€ Makefile                # Atalhos para comandos Docker e dbt
â””â”€â”€ requirements.txt        # DependÃªncias fixadas (Airflow, dbt, etc.)